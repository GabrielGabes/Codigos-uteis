```{r df ficticio}
source("~/Codigos úteis/Gerador de Dados Ficticios/dataframe.R", echo=F)
df
df = df[c('desfecho','desfecho_num','idade','idade_media','altura','alto_media','peso','imc','obeso','momento_1','momento_2','momento_3',
          'genero','tabagismo','doenca_cardiaca')]
df
```

```{r Analise geral}
tabelona = cont(df, names(df)[1]) %>% tabelinha_ajust()
tabelona = tabelona[FALSE, ]
for (coluna in names(df)){
  classe = class(df[[coluna]])[1]
  print(coluna)
  if (classe == "numeric"){
    tabelinha = summary_numerico_n_parametrico(df, coluna) %>% tabelinha_ajust()
  } else {
    tabelinha = cont(df, coluna) %>% tabelinha_ajust()
  }
  tabelona = rbind(tabelona, tabelinha)
}

tabelona %>% capture()
```


```{r Tabelas e teste de hipotese}
cross_table(df, 'desfecho')
```

```{r Analise Univariada}
cross_table_glm(df, 'desfecho')
```

# Analise Multi Variada

```{r Algoritmo de combinações}
busca_em_grade = function(variavel_dependente, variaveis_independentes){
  variaveis_independentes = sort(variaveis_independentes)
  # Gerando todas as combinações de variáveis independentes
  combinacoes = lapply(1:length(variaveis_independentes), function(x) combn(variaveis_independentes, x, simplify = FALSE))
  combinacoes = unlist( combinacoes, recursive = FALSE)
  
  # Ajustar modelos para cada combinação e armazenar os resultados
  resultados = data.frame(Combinacao = character(), stringsAsFactors = FALSE)
  
  for (comb in combinacoes) {
    existencia = any(tabelona$Combinacao %in% paste(comb, collapse = "+"))
    if (existencia == F){
      resultados = rbind(resultados, data.frame(Combinacao = paste(comb, collapse = "+"), stringsAsFactors = FALSE))
    }
  }
  resultados = resultados %>% distinct(Combinacao, .keep_all = TRUE)
  return(resultados)
}
```

# MODELOS DE CLASSIFICAÇÃO

```{r Pacotes}
library(pROC)
library(caret)
library(e1071)
library(car) #VIF
```


```{r Grade com combinações de variaveis_listas}
# Variaveis
lista_idade = c('idade','idade_media')
lista_altura = c('altura','alto_media')
lista_peso = c('peso','imc','obeso')
lista_momento = c('momento_1','momento_2','momento_3')

# Gerando todas as combinações possíveis # Colapsando cada combinação em uma única string
combinacoes = expand.grid(lista_idade, lista_altura, lista_peso, lista_momento)
combinacoes = data.frame(lapply(combinacoes, as.character), stringsAsFactors = FALSE)
combinacoes

# Preparando
variavel_dependente = 'desfecho'
colunas = names(df)
variaveis_independentes = colunas[!colunas %in% c('desfecho','desfecho_num' , lista_idade, lista_altura, lista_peso, lista_momento)]
variaveis_independentes
```

```{r}
names(df) 
```


```{r Aplicando Algoritmo de combinações}
# Criando todas combinações
variaveis_independentes = sort(variaveis_independentes)
tabelona = data.frame(Combinacao = character(), stringsAsFactors = FALSE)

for (linha in 1:nrow(combinacoes)){
  variveis_comb = append(variaveis_independentes, combinacoes[linha,] %>% as.character())
  tabela = busca_em_grade(variavel_dependente, variveis_comb)
  tabelona = rbind(tabelona, tabela)
}
tabelona = tabelona %>% distinct(Combinacao, .keep_all = TRUE)

# Criando Dataframe com todas combinações
num_linhas = nrow(tabelona)
novas_colunas = data.frame(
  tp = rep(NA, num_linhas), tn = rep(NA, num_linhas), fp = rep(NA, num_linhas), fn = rep(NA, num_linhas),
  Acuracia = rep(NA, num_linhas), Precisao = rep(NA, num_linhas), Especificidade = rep(NA, num_linhas), 
  AUC_ROC = rep(NA, num_linhas), 
  Pseudo_R2_McFadden = rep(NA, num_linhas), Pseudo_R2_Nagelkerke = rep(NA, num_linhas), 
  AIC = rep(NA, num_linhas), BIC = rep(NA, num_linhas), 
  VIF = rep(NA, num_linhas),
  controle = rep(NA, num_linhas),
  stringsAsFactors = FALSE
)
tabelona = cbind(tabelona, novas_colunas)
tabelona
```

ESTA SEMPRE ATENTO NA VARIAVEL DESFECHO
- sempre deve ser binaria com valores 0 ou 1

```{r}
variavel_dependente = 'desfecho'
linha = 10
vars_independentes = tabelona$Combinacao[linha]

formula_texto = paste0(variavel_dependente, "~", vars_independentes)
formula_do_modelo = as.formula(formula_texto)

# Modelo
modelo = glm(formula=formula_do_modelo, family = binomial(), data = df) #, weights = ps_ipw$weights)
metricas_de_avaliacao_glm( df, modelo, variavel_dependente, tabelona$Combinacao[linha] )

```


```{r Aplicando algorimo de grade}
variavel_dependente = 'desfecho'

for (linha in 1:nrow(tabelona)){
  controle = tabelona$controle[linha]
  if (controle == 'erro' | is.na(controle)){
    
    vars_independentes = tabelona$Combinacao[linha]
    formula_texto = paste0(variavel_dependente, "~", vars_independentes)
    formula_do_modelo = as.formula(formula_texto)
    
    # Modelo
    modelo = glm(formula=formula_do_modelo, family = binomial(), data = df) #, weights = ps_ipw$weights)
    
    tabelona[linha,] = c(tabelona$Combinacao[linha], 
                         metricas_de_avaliacao_glm( df, modelo, variavel_dependente, tabelona$Combinacao[linha] ) )
  } 
}

# Ultimas alterações
tabelona[, c(-1,-ncol(tabelona))] <- lapply(tabelona[, c(-1,-ncol(tabelona)) ], as.double)

tabelona$controle[tabelona$tp == 0 | tabelona$tn == 0] = 'inutil'
cont(tabelona, 'controle')

tabelona
```

```{r}
tabelona %>% filter(controle == 'erro')
```


```{r Analisando}

teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'idade')), 'AUC_ROC', cor_esc = 1) + labs(title='idade')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'altura')), 'AUC_ROC', cor_esc = 2) + labs(title='altura')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'peso')), 'Acuracia', cor_esc = 3) + labs(title='peso')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'genero')), 'Acuracia', cor_esc = 4) + labs(title='genero')

teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_1')), 'Acuracia', cor_esc = 5) + labs(title='momento_1')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_2')), 'Acuracia', cor_esc = 6) + labs(title='momento_2')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_3')), 'Acuracia', cor_esc = 7) + labs(title='momento_3')

```


###############################################################################


# MODELOS DE REGRESSÃO

```{r Grade com combinações de variaveis_listas}
# Variaveis
lista_idade = c('idade','idade_media')
lista_altura = c('altura','alto_media')
lista_peso = c('peso','imc','obeso')
lista_momento = c('momento_1','momento_2','momento_3')

# Gerando todas as combinações possíveis # Colapsando cada combinação em uma única string
combinacoes = expand.grid(lista_idade, lista_altura, lista_peso, lista_momento)
combinacoes = data.frame(lapply(combinacoes, as.character), stringsAsFactors = FALSE)
combinacoes

# Preparando
variavel_dependente = 'desfecho_num'
colunas = names(df)
variaveis_independentes = colunas[!colunas %in% c('desfecho','desfecho_num' , lista_idade, lista_altura, lista_peso, lista_momento)]
variaveis_independentes
```


```{r Aplicando Algoritmo de combinações}
# Criando todas combinações
variaveis_independentes = sort(variaveis_independentes)
tabelona = data.frame(Combinacao = character(), stringsAsFactors = FALSE)

for (linha in 1:nrow(combinacoes)){
  variveis_comb = append(variaveis_independentes, combinacoes[linha,] %>% as.character())
  tabela = busca_em_grade(variavel_dependente, variveis_comb)
  tabelona = rbind(tabelona, tabela)
}
tabelona = tabelona %>% distinct(Combinacao, .keep_all = TRUE)

# Criando Dataframe com todas combinações
num_linhas = nrow(tabelona)
novas_colunas = data.frame( 
  MAE = rep(NA, num_linhas), MSE = rep(NA, num_linhas), RMSE = rep(NA, num_linhas), MAPE = rep(NA, num_linhas),
  r_squared = rep(NA, num_linhas), r_squared_adj = rep(NA, num_linhas),
  aic = rep(NA, num_linhas), bic = rep(NA, num_linhas), 
  VIF = rep(NA, num_linhas),
  controle = rep(NA, num_linhas),
  stringsAsFactors = FALSE
)
tabelona = cbind(tabelona, novas_colunas)
tabelona
```


```{r}
variavel_dependente = 'desfecho_num'
linha = 10
vars_independentes = tabelona$Combinacao[linha]

formula_texto = paste0(variavel_dependente, "~", vars_independentes)
formula_do_modelo = as.formula(formula_texto)

# Modelo
modelo = lm(formula=formula_do_modelo, data = df)
metricas_de_avaliacao_regressao( df, modelo, variavel_dependente, tabelona$Combinacao[linha] )
```


```{r Aplicando algorimo de grade}
variavel_dependente = 'desfecho_num'

for (linha in 1:nrow(tabelona)){
  controle = tabelona$controle[linha]
  if (controle == 'erro' | is.na(controle)){
    
    vars_independentes = tabelona$Combinacao[linha]
    formula_texto = paste0(variavel_dependente, "~", vars_independentes)
    formula_do_modelo = as.formula(formula_texto)
    
    # Modelo
    modelo = lm(formula=formula_do_modelo, data = df)
    
    tabelona[linha,] = c(tabelona$Combinacao[linha], 
                         metricas_de_avaliacao_regressao( df, modelo, variavel_dependente, tabelona$Combinacao[linha] ) )
  }
}
# Ultimos ajustes
tabelona[, c(-1,-ncol(tabelona))] <- lapply(tabelona[, c(-1,-ncol(tabelona)) ], as.double)

#tabelona$controle[tabelona$tp == 0 | tabelona$tn == 0] = 'inutil'
cont(tabelona, 'controle')

tabelona
```


```{r}

teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'idade')), 'MAE', cor_esc = 1) + labs(title='idade')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'altura')), 'MAE', cor_esc = 2) + labs(title='altura')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'peso')), 'MAE', cor_esc = 3) + labs(title='peso')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'genero')), 'MAE', cor_esc = 4) + labs(title='genero')

teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_1')), 'MAE', cor_esc = 5) + labs(title='momento_1')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_2')), 'MAE', cor_esc = 6) + labs(title='momento_2')
teste_normalidade(tabelona %>% filter(str_detect(Combinacao, 'momento_3')), 'MAE', cor_esc = 7) + labs(title='momento_3')

```


```{r Grafico Forest Plot}

fore_plot = function(data_df, variavel_dependente, vars_independentes){
  formula_texto = paste0(variavel_dependente, "~", vars_independentes)
  formula_do_modelo = as.formula(formula_texto)
  vars_independentes_lista = unlist(strsplit(vars_independentes, split = "\\+"))
  
  modelo = lm(formula=formula_do_modelo, data = data_df)
  
  summary_modelo = modelo %>% summary()
  coeficientes = summary_modelo$coefficients
  
  ic = confint(modelo, method = "Wald")
  #ic <- ic[-c(1), ]
  
  estimadores = cbind(coeficientes, ic)[-1,] %>% as.data.frame()
  estimadores[["Variable"]] = rownames(estimadores)
  
  names(estimadores)[names(estimadores) == "Pr(>|t|)"] = "p_value"
  names(estimadores)[names(estimadores) == "2.5 %"] = "IC_0"
  names(estimadores)[names(estimadores) == "97.5 %"] = "IC_1"
  estimadores = apply_retorne_p(estimadores, 'p_value')
  estimadores$estimador = NA
  for (i in 1:nrow(estimadores)){ 
    estimadores$estimador[i] = paste0( rround(estimadores$Estimate[i],2), 
                                       ' (', rround(estimadores$IC_0[i],2), 
                                       ' to ', 
                                       rround(estimadores$IC_1[i],2), ')')
    estimadores$Variable[i] = adicionar_quebra_de_linha(estimadores$Variable[i], 20)
  }
  rownames(estimadores) = 1:nrow(estimadores)
  
  # Grafico Estimadores
  plot1 = ggplot(estimadores, aes(y = Variable, x = Estimate)) + #
    geom_point(shape = 18, size = 5, position = position_dodge(width = 0.5)) +  
    geom_errorbarh(aes(xmin = IC_0, xmax = IC_1), 
                   height = 0.25, position = position_dodge(width = 0.5)) +
    geom_vline(xintercept = 0, color = "tomato", linetype = "dashed", cex = 1, alpha = 0.5) +
    labs(title=' ', x="Estimators of Linear Model (95% CI)", y='PEEP') +
    theme_bw() +
    theme(legend.position = 'none',
          plot.title = element_text(hjust = 0.5, size=12),
          panel.border = element_blank(),
          panel.background = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(), 
          axis.line = element_line(colour = "black"),
          axis.text.y = element_text(size = 12, colour = "black"),
          axis.text.x.bottom = element_text(size = 12, colour = "black"),
          axis.title.x = element_text(size = 12, colour = "black")) +
    theme_bw() + guides(color = FALSE)
    scale_x_continuous(trans='log10') #+ geom_text(aes(label = pvalor))
  
  # Grafico em branco
  table_base = ggplot(estimadores, aes(y=Variable)) +
    labs(y=NULL) + 
    theme_bw() +
    theme(legend.position = 'none',
          plot.title = element_text(hjust = 0.5, size=12), 
          axis.text.x = element_text(color="white", hjust = -3, size = 25), 
          ## This is used to help with alignment
          axis.line = element_blank(),
          axis.text.y = element_blank(), 
          axis.ticks = element_blank(),
          axis.title.y = element_blank(), 
          panel.background = element_blank(), 
          panel.border = element_blank(), 
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(), 
          plot.background = element_blank())
  
  # Legenda 1
  tab1 = table_base +
    geom_text(aes(x = 1, label = p_value, fontface = "bold"), 
              size = 4, position = position_dodge(width = 0.5)) + #, color = momento
    labs(title="P-valor", x=NULL)
  
  # Legenda 2
  tab2 = table_base + 
    labs(title = "space") +
    geom_text(aes(x = 1, label = estimador, fontface = "bold"), 
              size = 4, position = position_dodge(width = 0.5)) + #, color = momento
    labs(title="Estimator", x=NULL)
  
  # lay =  matrix(c(1,1,1,1,1,1,1,1,1,2,3,3), nrow = 1)
  # grid = grid.arrange(plot1, tab1, tab2, layout_matrix = lay)
  # return(grid)
  return(plot1) # apenas o grafico
}

for (linha in sample(c(100:nrow(tabelona)), 10) ){
  fore_plot(df, 'desfecho_num', tabelona$Combinacao[linha]) %>% print()
}

```

# Modelos Stepwise

## Tipos de modelos
1. Forward Selection (Seleção Progressiva)
Começa com um modelo sem variáveis (modelo nulo).
Adiciona a variável que mais melhora o ajuste do modelo, conforme determinado por um critério de seleção (como AIC, BIC, ou valor p).
Repetidamente adiciona variáveis, uma de cada vez, até que nenhuma variável adicional melhore significativamente o modelo.

2. Backward Elimination (Eliminação Regressiva)
Começa com um modelo que inclui todas as variáveis preditoras.
Remove a variável que menos contribui para o modelo, conforme determinado por um critério de seleção.
Repetidamente remove variáveis, uma de cada vez, até que todas as variáveis restantes contribuam significativamente para o modelo.

3. Stepwise Regression (Regressão por Etapas)
Combina os métodos de seleção progressiva e eliminação regressiva.
Começa com um modelo vazio (forward stepwise) ou com todas as variáveis (backward stepwise).
Em cada etapa, uma variável pode ser adicionada ou removida, dependendo de qual ação melhora mais o modelo segundo o critério de seleção.
Continua até que nenhuma adição ou remoção de variável melhore significativamente o modelo.

## Critérios de Seleção:
1. AIC (Akaike Information Criterion): Penaliza a complexidade do modelo para evitar overfitting.

2. BIC (Bayesian Information Criterion): Semelhante ao AIC, mas penaliza modelos complexos mais severamente.

3. Valor p: Utilizado em testes de hipóteses para determinar a significância estatística das variáveis.

```{r Definindo Modelo Completo}
variavel_dependente = 'desfecho_num'

# Ajustar um modelo inicial com todas as variáveis
modelo_completo <- lm(desfecho_num ~ alto_media+doenca_cardiaca+genero+idade_media+momento_3+obeso+tabagismo, data = df)

metricas_de_avaliacao_regressao( df, modelo_completo, variavel_dependente)

modelo_completo$coefficients
```


```{r AIC}

# Aplicar a regressão stepwise (AIC) - backward elimination
modelo_stepwise_AIC_backward <- step(modelo_completo, direction = "backward")
#summary(modelo_stepwise_AIC_backward)

# Aplicar a regressão stepwise (AIC) - forward selection
modelo_nulo <- lm(desfecho_num ~ 1, data = df)
modelo_stepwise_AIC_forward <- step(modelo_nulo, direction = "forward", scope = formula(modelo_completo))
#summary(modelo_stepwise_AIC_forward)

# Aplicar a regressão stepwise (AIC) - stepwise
modelo_stepwise_AIC_both <- step(modelo_completo, direction = "both")
#summary(modelo_stepwise_AIC_both)

metricas_de_avaliacao_regressao( df, modelo_stepwise_AIC_backward, variavel_dependente )
metricas_de_avaliacao_regressao( df, modelo_stepwise_AIC_forward, variavel_dependente )
metricas_de_avaliacao_regressao( df, modelo_stepwise_AIC_both, variavel_dependente )

modelo_stepwise_AIC_backward$coefficients
modelo_stepwise_AIC_forward$coefficients
modelo_stepwise_AIC_both$coefficients

```


```{r BIC}

# Número de observações
n <- nrow(df)

# Aplicar a regressão stepwise (BIC) - backward elimination
modelo_stepwise_BIC_backward <- stepAIC(modelo_completo, direction = "backward", k = log(n))
summary(modelo_stepwise_BIC_backward)

# Aplicar a regressão stepwise (BIC) - forward selection
modelo_nulo <- lm(desfecho_num ~ 1, data = df)
modelo_stepwise_BIC_forward <- stepAIC(modelo_nulo, direction = "forward", scope = list(lower = formula(modelo_nulo), upper = formula(modelo_completo)), k = log(n))
summary(modelo_stepwise_BIC_forward)

# Aplicar a regressão stepwise (BIC) - stepwise
modelo_stepwise_BIC_both <- stepAIC(modelo_completo, direction = "both", k = log(n))
summary(modelo_stepwise_BIC_both)

metricas_de_avaliacao_regressao( df, modelo_stepwise_BIC_backward, variavel_dependente )
metricas_de_avaliacao_regressao( df, modelo_stepwise_BIC_forward, variavel_dependente )
metricas_de_avaliacao_regressao( df, modelo_stepwise_BIC_both, variavel_dependente )

modelo_stepwise_BIC_backward$coefficients
modelo_stepwise_BIC_forward$coefficients
modelo_stepwise_BIC_both$coefficients

```


```{r P-valor - Incompleto}

# Critério de seleção teste F

step(modelo_completo, direction = "backward", test = "F")

#### Função chatgpt # virou um loop infinito

# # Função para backward elimination baseada em valor p
# backward_elimination_p <- function(modelo, limiar = 0.05) {
#   while(TRUE) {
#     p_valores <- summary(modelo)$coefficients[,4]
#     p_valor_max <- max(p_valores[-1])
#     if(p_valor_max < limiar) break
#     variavel_remover <- names(p_valores)[which.max(p_valores[-1])+1]
#     formula_atual <- as.formula(paste(". ~ . -", variavel_remover))
#     modelo <- update(modelo, formula_atual)
#   }
#   return(modelo)
# }
# 
# # Aplicar a regressão stepwise (valor p) - backward elimination
# modelo_stepwise_p <- backward_elimination_p(modelo_completo)
# 
# # Sumário do modelo final
# summary(modelo_stepwise_p)

#### Função própria

# Function has.interaction checks whether x is part of a term in terms
# terms is a vector with names of terms from a model
has.interaction <- function(x,terms){
    out <- sapply(terms, function(i){
        sum(1-(strsplit(x,":")[[1]] %in% strsplit(i,":")[[1]]))==0
    })
    return(sum(out)>0)
}

# Function Model.select
# model is the lm object of the full model
# keep is a list of model terms to keep in the model at all times
# sig gives the significance for removal of a variable. Can be 0.1 too (see SPSS)
# verbose=T gives the F-tests, dropped var and resulting model after 
model.select <- function(model,keep,sig=0.05,verbose=F){
      counter=1
      # check input
      if(!is(model,"lm")) stop(paste(deparse(substitute(model)),"is not an lm object\n"))
      # calculate scope for drop1 function
      terms <- attr(model$terms,"term.labels")
      if(missing(keep)){ # set scopevars to all terms
          scopevars <- terms
      } else{            # select the scopevars if keep is used
          index <- match(keep,terms)
          # check if all is specified correctly
          if(sum(is.na(index))>0){
              novar <- keep[is.na(index)]
              warning(paste(
                  c(novar,"cannot be found in the model",
                  "\nThese terms are ignored in the model selection."),
                  collapse=" "))
              index <- as.vector(na.omit(index))
          }
          scopevars <- terms[-index]
      }

      # Backward model selection : 

      while(T){
          # extract the test statistics from drop.
          test <- drop1(model, scope=scopevars,test="F")

          if(verbose){
              cat("-------------STEP ",counter,"-------------\n",
              "The drop statistics : \n")
              print(test)
          }

          pval <- test[,dim(test)[2]]

          names(pval) <- rownames(test)
          pval <- sort(pval,decreasing=T)

          if(sum(is.na(pval))>0) stop(paste("Model",
              deparse(substitute(model)),"is invalid. Check if all coefficients are estimated."))

          # check if all significant
          if(pval[1]<sig) break # stops the loop if all remaining vars are sign.

          # select var to drop
          i=1
          while(T){
              dropvar <- names(pval)[i]
              check.terms <- terms[-match(dropvar,terms)]
              x <- has.interaction(dropvar,check.terms)
              if(x){i=i+1;next} else {break}              
          } # end while(T) drop var

          if(pval[i]<sig) break # stops the loop if var to remove is significant

          if(verbose){
             cat("\n--------\nTerm dropped in step",counter,":",dropvar,"\n--------\n\n")              
          }

          #update terms, scopevars and model
          scopevars <- scopevars[-match(dropvar,scopevars)]
          terms <- terms[-match(dropvar,terms)]

          formul <- as.formula(paste(".~.-",dropvar))
          model <- update(model,formul)

          if(length(scopevars)==0) {
              warning("All variables are thrown out of the model.\n",
              "No model could be specified.")
              return()
          }
          counter=counter+1
      } # end while(T) main loop
      return(model)
}

model.select(modelo_completo)

##### Aplicação com pacote RMS

require(rms)
model1 <- ols(desfecho_num ~ alto_media+doenca_cardiaca+genero+idade_media+momento_3+obeso+tabagismo, data = df)
model2 <- fastbw(fit=model1, rule="p", sls=0.05)
model2

```


```{r}

```


```{r}
```


```{r}
```
